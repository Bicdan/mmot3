{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 3 \n",
    "\n",
    "# Классификация с использованием BERT  и Transfer learning\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Бикметов Данил Наильевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "\n",
    "В этом задании вы будете определять категории товара по данным из чеков, предоставленным в соревновании [Data Fusion Context](https://boosters.pro/championship/data_fusion/data).\n",
    "\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Transformers](https://github.com/huggingface/transformers).\n",
    " - [Tokenizers](https://github.com/huggingface/tokenizers).\n",
    "\n",
    "Данные лежат в архиве data.zip, в котором лежит файл `data.csv`, содержащий тексты и соответствующие им категории товаров. Все объекты поделены между train, test, val и unsupervised. Для unsupervised объектов категории товаров недоступны. \n",
    "\n",
    "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/1AHs7qJYg2tc8zblGlT0Dpe50e6RW-gAW/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных (2 балла)\n",
    "\n",
    "Классические методы NLP (например, как мы делали в первом и втором домашнем задании) преобразуют тексты в списки индексов следующим образом:\n",
    "1. \"Очистка текста\" от плохих символов, приводим (или не приводим) текст к нижнему регистру.\n",
    "2. Текст делится по пробелам на слова.\n",
    "3. По полученной коллекции текстов строится словарь вида \"слово -> индекс\", редкие слова выбрасываются, стопслова иногда тоже\n",
    "4. Побитый на слова текст превращается в список индексов с помощью этого словаря.\n",
    "\n",
    "Для трансформеров схема выглядит немного по-другому — используются более продвинутые методы токенизации типа `wordpiece, bpe, sentencepiece`. Основное концептуальное отличие — текст делится не только на слова по пробелам, но и сами слова делятся на \"подслова\" (читай subwords). Это верно для BPE и wordpiece, а sentencepiece вообще не учитывает пробелы. Более подробно ознакомиться с этими методами токенизации можно в наших лекциях.\n",
    "\n",
    "В данном задании предлагается использовать wordpiece токенизатор, который использовали в оригинальной статье про BERT. Построить его можно с помощью библиотеки `tokenizers`:\n",
    "1. Считайте данные с помощью `pandas`\n",
    "2. Используя метод `tokenizers.BertWordPieceTokenizer.train` и список сырых текстов постройте токенизатор. Используйте нижний регистр (lowercase), чистый текст (clean_text), без акцентов (strip_accents), размера словаря 30000 (vocab_size).\n",
    "3. Сохраните построенный токенизатор (метод `tokenizer.save_model`) и создайте объект класса `transformers.BertTokenizerFast`, который работает быстрее стандартной реализации, но не позволяет её обучать.\n",
    "\n",
    "**Важно:** нужно при обучении c помощью параметра `special_tokens` завести индексы для токенов `[PAD], [UNK], [CLS], [SEP], [MASK]`, которые понадобятся нам дальше для обучения и использования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('task3_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_path, \n",
    "            strip_accents=True, \n",
    "            clean_text=True, \n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_path: путь к словарю\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            Подгружает токенизатор с помощью BertTokenizerFast.\n",
    "        \"\"\"\n",
    "        self._tokenizer = BertTokenizerFast(\n",
    "            vocab_file=vocab_path,\n",
    "            strip_accents=strip_accents,\n",
    "            clean_text=clean_text,\n",
    "            do_lower_case=lowercase\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(\n",
    "            cls,\n",
    "            corpus,\n",
    "            corpus_save_path,\n",
    "            tokenizer_save_path,\n",
    "            tokenizer_name,\n",
    "            vocab_size=30000,\n",
    "            min_frequency=2,\n",
    "            strip_accents=True,\n",
    "            clean_text=True,\n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: список текстов\n",
    "            corpus_save_path: временный путь для сохранения текстов в текстовом файле\n",
    "            tokenizer_save_path: путь для сохранения файлов токенизатора\n",
    "            tokenizer_name: название токенизатора, влияет на названия файлов токенизатора\n",
    "            vocab_size: размер словаря\n",
    "            min_frequency: минимальная частота элемента в словаре\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            С помощью списка сырых текстов формирует токенизатор\n",
    "        \"\"\"\n",
    "        with open(corpus_save_path, 'w', encoding='utf-8') as f:\n",
    "            for text in corpus:\n",
    "                f.write(text + '\\n')\n",
    "\n",
    "        tokenizer = BertWordPieceTokenizer(\n",
    "            clean_text=clean_text,\n",
    "            strip_accents=strip_accents,\n",
    "            lowercase=lowercase,\n",
    "        )\n",
    "        tokenizer.train(\n",
    "            [corpus_save_path],\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "        os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "        tokenizer.save_model(tokenizer_save_path)\n",
    "\n",
    "        os.remove(corpus_save_path)\n",
    "\n",
    "        return cls(\n",
    "            vocab_path=os.path.join(tokenizer_save_path, 'vocab.txt'),\n",
    "            strip_accents=strip_accents,\n",
    "            clean_text=clean_text,\n",
    "            lowercase=lowercase\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: str. Сырой текст\n",
    "            \n",
    "            returns: list of ints. Список индексов\n",
    "            \n",
    "            C помощью метода .encode преобразует текст в индексы.\n",
    "        \"\"\"\n",
    "        return self._tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        \"\"\"\n",
    "            returns: индекс CLS токена\n",
    "        \"\"\"\n",
    "        return self._tokenizer.cls_token_id\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self._tokenizer.pad_token_id\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        return self._tokenizer.mask_token_id\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        return self._tokenizer.sep_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: размер словаря\n",
    "        \"\"\"\n",
    "        return self._tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте токенизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [row for row in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordpieceTokenizer.from_corpus(\n",
    "    corpus=corpus,\n",
    "    corpus_save_path='./corpus',\n",
    "    tokenizer_save_path='./tokenizer',\n",
    "    tokenizer_name='wordpiece'\n",
    ")\n",
    "\n",
    "tests.test_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам доступно довольно большое количество неразмеченных данных, которое можно использовать для предобучения модели. Мы рассмотрим две задачи предобучения:\n",
    "1. **Masked Language Modeling** — задача из BERT. Выбираем 15% слов, из них 80% заменяем на токен [MASK], 10% меняем на другие случайные слова, 10% оставляем как есть. Эти 15% слов предсказываем моделью. Вспомним пример из оригинальной статьи:\n",
    "    * Исходный текст: `my dog is hairy`\n",
    "    * Выбираем случайным образом 15% токенов для задачи. Допустим, выбрали четвертый токен - `hairy`\n",
    "    * В 80% случаев заменяем токен на `[MASK]`: `my dog is [MASK]`\n",
    "    * В 10% случаев на другой случайный токен: `my dog is apple`\n",
    "    * В 10% случаев оставляем неизменным: `my dog is hairy`\n",
    "    \n",
    "    \n",
    "2. **Sentence Order Prediction** — задача из ALBERT. Делим текст на два сегмента, с вероятностью 50% меняет сегменты местами. Предсказываем, в правильном ли порядке находятся сегменты.\n",
    "    * Текст: `the man went to the store. he bought a gallon of milk`\n",
    "    * Токенизируем и делим его на два сегмента: `the man went to the store` и `he bought a gallon of milk`\n",
    "    * C вероятностью 50% меняем их местами: `[CLS] he bought a gallon of milk [SEP] the man went to the store`\n",
    "    * С вероятностью 50% оставляем на месте: `[CLS] the man went to the store [SEP] he bought a gallon of milk`\n",
    "    * \"Левому\" сегменту соответствует нулевой индекс сегмента, \"правому\" - индекс 1\n",
    "\n",
    "Большая часть логики предобучения реализуется при подготовке данных.\n",
    "\n",
    "Реализуйте **PretrainDataset**, который токенизирует поданные сырые тексты и умеет возвращать для текста с конкретным индексом случайный сегмент длины, не большей чем `maxlen`. Логика для задачи **SOP** должна быть реализована в `__getitem__`: выбранный сегмент надо поделить на два равных сегмента, подбросить монетку, и с 50% вероятностью поменять сегменты местами. Нужно также добавить `[CLS]` и `[SEP]` токены.\n",
    "\n",
    "**hint:** чтобы существенно ускорить обучение (не потеряв при этом в качестве), после токенизации отсортируйте датасет по длине текстов.\n",
    "**hint:** токенизация датасета для предобучения занимает существенное время (5 минут), поэтому во время отладки стоит сделать её один раз и сохранить результат на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            corpus, \n",
    "            tokenizer, \n",
    "            minlen,\n",
    "            maxlen,\n",
    "            permute_prob=0.5, \n",
    "            verbose=False, \n",
    "            presort=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: list of strings. Список сырых текстов\n",
    "            tokenizer: токенизатор\n",
    "            minlen: минимально допустимая длина текста\n",
    "            permute_prob: вероятность, с которой два сегмента меняются местами (происходит swap)\n",
    "            maxlen: максимальная длина текста\n",
    "            verbose: вывод прогресса токенизации текстов с помощью tqdm\n",
    "            presort: отсортировать датасет по длинам токенизированных текстов (т.е. ds[0] выдает самый короткий текст)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._corpus = corpus\n",
    "        self._tokenizer = tokenizer\n",
    "        self._minlen = minlen\n",
    "        self._maxlen = maxlen\n",
    "        self._permute_prob = permute_prob\n",
    "        self._verbose = verbose\n",
    "        self._presort = presort\n",
    "\n",
    "        # Токенизация и предварительная обработка данных\n",
    "        self._tokenized_data = self.tokenize_corpus()\n",
    "\n",
    "        # Опциональная сортировка\n",
    "        if self._presort:\n",
    "            self._sorted_indices = np.argsort([len(tokens) for tokens in self._tokenized_data])\n",
    "        else:\n",
    "            self._sorted_indices = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._corpus)\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "            returns: tokenizer. Нужно для тестов\n",
    "        \"\"\"\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def set_maxlen(self, maxlen):\n",
    "        \"\"\"\n",
    "            maxlen: максимальная длина текста\n",
    "            \n",
    "            поставить новое максимальное значение длины\n",
    "        \"\"\"\n",
    "        self._maxlen = maxlen\n",
    "\n",
    "    def tokenize_corpus(self):\n",
    "        tokenized_data = []\n",
    "        for text in tqdm(self._corpus, desc=\"Tokenizing\", disable=not self._verbose):\n",
    "            tokens = self._tokenizer(text)\n",
    "            if len(tokens) >= self._minlen:\n",
    "                tokenized_data.append(tokens)\n",
    "        return tokenized_data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: \n",
    "                input_ids - тензор с индексами, \n",
    "                token_type_ids - тензор с сегментными айдишниками (0 у левого сегмента, 1 у правого),\n",
    "                permuted - был ли swap сегментов\n",
    "        \"\"\"\n",
    "        if self._presort:\n",
    "            idx = self._sorted_indices[idx]\n",
    "\n",
    "        tokens = self._tokenized_data[idx]\n",
    "\n",
    "        # Ограничение длины текста\n",
    "        if len(tokens) > self._maxlen:\n",
    "            tokens = tokens[:self._maxlen]\n",
    "\n",
    "        # Разделение на два сегмента\n",
    "        split_point = random.randint(1, len(tokens) - 1)\n",
    "        segment1 = tokens[:split_point]\n",
    "        segment2 = tokens[split_point:]\n",
    "\n",
    "        # Монетка для определения, менять ли сегменты местами\n",
    "        permuted = random.random() < self._permute_prob\n",
    "        if permuted:\n",
    "            segment1, segment2 = segment2, segment1\n",
    "\n",
    "        # Объединяем сегменты с добавлением токенов [CLS] и [SEP]\n",
    "        input_ids = [self._tokenizer.cls_token_id] + segment1 + [self._tokenizer.sep_token_id] + segment2 + [self._tokenizer.sep_token_id]\n",
    "        # Сегментные айдишники\n",
    "        token_type_ids = [0] * (len(segment1) + 2) + [1] * (len(segment2) + 1)\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(token_type_ids), permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно ограничить снизу. Нет смысла рассматривать слишком короткие тексты (например, единичную длину), для которых задачи предобучения вообще не работают. Длинные тексты более эффективны для задач типа MLM, так как у модели больше контекста для предсказания и больше таргетов на один объект.\n",
    "\n",
    "Разумный способ определить минимальную длину текстов для MLM  — подобрать такую минимальную длину, чтобы вероятность замаскировать хотя бы одно слово в тексте была больше заданного порога.\n",
    "\n",
    "Т.е. если мы каждое слово маскируем с вероятностью 15%, какой длины должен быть текст, чтобы с вероятностью $\\geqslant$ 50% было замаскировано хотя бы одно слово?\n",
    "\n",
    "Используйте ответ на данный вопрос как минимальную допустимую длину текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Минимальная длина текста: 5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_min_length(p_mask, T):\n",
    "    return math.ceil(math.log(1 - T) / math.log(1 - p_mask))\n",
    "\n",
    "p_mask = 0.15  # Вероятность маскировки одного слова\n",
    "T = 0.5  # Желаемый порог вероятности замаскировать хотя бы одно слово\n",
    "\n",
    "minlen = calculate_min_length(p_mask, T)\n",
    "print(f\"Минимальная длина текста: {minlen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасет (с произвольным разумным значением maxlen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ds = PretrainDataset(\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer, \n",
    "    minlen=minlen,\n",
    "    maxlen=100,\n",
    "    permute_prob=0.5, \n",
    "    verbose=False, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно как-то ограничить сверху. Иначе, если встретится какой-то очень-очень длинный текст, он не поместится в видеопамять. Самый простой способ определить ограничение по длине  — после токенизации построить гистограмму длин (например, используя **sns.distplot**) и методом пристального взгляда определить разумное ограничение длины. Другой вариант  — взять большое значение квантили.\n",
    "\n",
    "**Вопрос:** какая максимальная длина текста подходит для этого датасета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/1cp87znx719_9gr4btbj2y94_h5ks0/T/ipykernel_73550/1945019452.py:7: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(text_lengths, kde=False, bins=30)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "text_lengths = [len(tokens) for tokens in ds._tokenized_data]\n",
    "\n",
    "sns.distplot(text_lengths, kde=False, bins=30)\n",
    "plt.title(\"Распределение длин текстов\")\n",
    "plt.xlabel(\"Длина текста\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присвойте максимально допустимое значение длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_maxlen(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для задачи **SOP** мы готовим данные при индексации датасета `PretrainDataset`, то маскирование для задачи **MLM** удобней делать в Collator'е в тензорном виде.\n",
    "\n",
    "Как с вероятностью `15%` заменить в тензоре `input_ids` значения на `0`: \n",
    "\n",
    "1. `mask = torch.rand(input_ids.shape) < 0.15`\n",
    "2. `input_ids = torch.where(mask, 0, input_ids)`\n",
    "\n",
    "Как сгенерировать случайные элементы словаря на каждый элемент батча: `torch.randint_like(input_ids, low=num_special_tokens, high=self._tokenizer.vocab_size)`.\n",
    "\n",
    "В `Collator` нужно также:\n",
    "1. сделать паддинг.\n",
    "2. из (примерно) 15% выбранных токенов 10% поменять на случайные и 10% оставить в исходном виде, остальные замаскировать\n",
    "3. сформировать таргеты. Нам нужно понимать, какие именно 15% токенов мы выбрали для предсказания + какие были исходные метки для них.\n",
    "\n",
    "Важно: `[CLS]` и другие специальные токены токены маскировать не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            non_target_idx=-100,\n",
    "            mask_prob=0.15,\n",
    "            random_prob=0.1,\n",
    "            keep_unchanged_prob=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            tokenizer: токенизатор\n",
    "            non_target_idx: значение для индексов, не использующихся как таргеты. \n",
    "                Используйте его, чтобы пометить \"не таргет\" токены\n",
    "            mask_prob: вероятность выбрать индекс как таргет\n",
    "            random_prob: вероятность для уже выбранного индекса поменять его на случайное значение вместо маскирования\n",
    "            keen_unchanged_prob: вероятность оставить индекс в исходном виде вместо маскирования\n",
    "        \"\"\"\n",
    "        self._tokenizer = tokenizer\n",
    "        self._non_target_idx = non_target_idx\n",
    "        self._mask_prob = mask_prob\n",
    "        self._random_prob = random_prob\n",
    "        self._keep_unchanged_prob = keep_unchanged_prob\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "            batch: список вида [ds[i] for i in [12, 3, 2, 5]]\n",
    "            \n",
    "            returns: \n",
    "                input_ids: испорченные входные индексы токенов с замаскированными значениями\n",
    "                token_type_ids: сегментные эмбеддинги\n",
    "                labels: истинные значения входных индексов, как таргеты\n",
    "                permuted: был ли свап сегментов\n",
    "        \"\"\"\n",
    "        input_ids, token_type_ids, permuted = zip(*[(ds[0], ds[1], ds[2]) for ds in batch])\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "        token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
    "        permuted = torch.tensor(permuted, dtype=torch.float32).view(-1, 1)  # Преобразуем в тензор и добавим размерность\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Маскирование токенов\n",
    "        mask = torch.rand(input_ids.shape) < self._mask_prob\n",
    "        non_target_mask = mask & (input_ids != self._tokenizer.cls_token_id) & (input_ids != self._tokenizer.sep_token_id)\n",
    "        random_mask = (non_target_mask & (torch.rand(input_ids.shape) < self._random_prob)).bool()\n",
    "        keep_unchanged_mask = (non_target_mask & ~random_mask & (torch.rand(input_ids.shape) < self._keep_unchanged_prob)).bool()\n",
    "\n",
    "        input_ids[non_target_mask] = self._non_target_idx\n",
    "        input_ids[random_mask] = torch.randint_like(input_ids[random_mask], low=self._tokenizer.pad_token_id, high=self._tokenizer.vocab_size)\n",
    "        # Также можно использовать self._tokenizer.mask_token_id вместо self._tokenizer.pad_token_id\n",
    "        # в случае, если ваш токенизатор поддерживает токен для маскирования\n",
    "\n",
    "        labels[~non_target_mask] = self._non_target_idx\n",
    "\n",
    "        return input_ids, token_type_ids, labels, permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте `collator` и `dataloader`. Для предобучения предлагается использовать большой `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer, non_target_idx=-100)\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    collate_fn=collator, \n",
    "    batch_size=256, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n",
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_collator(ds, collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Создание модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели мы будем использовать энкодер трансформера точь-в-точь в таком же виде, как его использовали в оригинальной статье про BERT. \n",
    "\n",
    "Прежде чем начать писать составляющие энкодера, обсудим инициализацию весов. Для трансформера нам понадобится большое количество линейных слов (`nn.Linear`), у которых для инициализации по дефолту используется равномерное распределение и отсутствует зануление bias'ов: $$\\text{Uniform}\\left(-\\frac{1}{\\sqrt{N_{\\text{in\\_features}}}}, \\frac{1}{\\sqrt{N_{\\text{in\\_features}}}}\\right).$$\n",
    "\n",
    "В оригинальной статье про BERT для весов используется **TruncatedNormal** со стандартным отклонением 0.02, bias'ы инициализируются нулями и модель обучается значительно лучше (это можно в ходе домашнего задания проверить).\n",
    "\n",
    "Поэтому, после создания линейных слоев и матрицы эмбеддингов, необходимо в явном виде вызывать для них TruncatedNormal инициализацию:\n",
    "\n",
    "1. `layer = ...`\n",
    "2. `nn.init.trunc_normal_(layer.weight.data, std=0.02, a=-2 * 0.02, b=2 * 0.02)`.\n",
    "\n",
    "Для линейных слоев нужно также вызывать `layer.bias.data.zero_()`.\n",
    "\n",
    "\n",
    "**TruncatedNormal** распределение отличается от нормального тем, что если величины выходят за границы отрезка [a, b], для этих величин повторно происходит сэмплирование до тех пор, пока они не попадут в нужный отрезок. Для BERT stddev = 0.02:\n",
    "\n",
    "$$[a; b] = [- 2  \\cdot \\text{stddev}; 2 \\cdot \\text{stddev}].$$\n",
    "\n",
    "Напишите функцию для инициализации линейных слоев и матрицы эмбеддингов **TruncatedNormal** распределением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def init_layer(layer, initializer_range=0.02, zero_out_bias=True):\n",
    "    \"\"\"\n",
    "        layer: наследник nn.Module, т.е. слой в pytorch\n",
    "        initializer_range: stddev для truncated normal\n",
    "        zero_out_bias: True для линейных слоев, False для матрицы эмбеддингов\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # Инициализация весов линейного слоя TruncatedNormal распределением\n",
    "        nn.init.trunc_normal_(layer.weight.data, std=initializer_range)\n",
    "        if zero_out_bias:\n",
    "            # Обнуление смещения (bias) для линейных слоев\n",
    "            layer.bias.data.zero_()\n",
    "    elif isinstance(layer, nn.Embedding):\n",
    "        # Инициализация матрицы эмбеддингов TruncatedNormal распределением\n",
    "        nn.init.trunc_normal_(layer.weight.data, std=initializer_range)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported layer type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к созданию энкодера трансформера.\n",
    "\n",
    "<img src=\"images/transformer.png\" width=500 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем слой, создающий входные векторы токенов. Для этого нам нужны:\n",
    "1. Эмбеддинги токенов (`nn.Embedding`)\n",
    "2. Позиционные эмбеддинги (можно либо использовать `nn.Embedding`, либо явно создать матрицу эмбеддингов с помощью `nn.Parameter`)\n",
    "3. Сегментные эмбеддинги\n",
    "\n",
    "Эти три сущности складываются, затем идет layernorm и dropout.\n",
    "\n",
    "<img src=\"images/bert_input.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size, \n",
    "            hidden_size, \n",
    "            max_seqlen,\n",
    "            dropout_prob=0., \n",
    "            type_vocab_size=2,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_size: размер словаря\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            max_seqlen: количество позиционных эмбеддингов\n",
    "            dropout_prob: вероятность дропаута в конце слоя\n",
    "            type_vocab_size: количество сегментных эмбеддингов\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self._position_embeddings = nn.Embedding(max_seqlen, hidden_size)\n",
    "        self._token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        self._LayerNorm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "        self._dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def get_token_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает слой с матрицей эмбеддингов для токенов. Нужен для MLM головы\n",
    "        \"\"\"\n",
    "        return self._token_embeddings\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: тензор с индексами токенов\n",
    "            token_type_ids: сегментные индексы\n",
    "            \n",
    "            returns: эмбеддинги токенов\n",
    "        \"\"\"\n",
    "        token_embeddings = self._token_embeddings(input_ids)\n",
    "\n",
    "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self._position_embeddings(position_ids)\n",
    "\n",
    "        # Если не предоставлены сегментные индексы, используем нули\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        token_type_embeddings = self._token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # Сложение эмбеддингов токенов, позиций и сегментов\n",
    "        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # LayerNorm и применение Dropout\n",
    "        embeddings = self._LayerNorm(embeddings)\n",
    "        embeddings = self._dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7746560.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_embeddings(BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит одноголовый **SelfAttention**:\n",
    "    \n",
    "<img src=\"images/attention.png\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит многоголовый (multihead) **SelfAttention:**\n",
    "\n",
    "<img src=\"images/multihead.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация **MultiHeadSelfAttention** — самая сложная часть энкодера. Дальше будет проще :)\n",
    "\n",
    "1. Принимаем на вход посл-ть векторов для каждого объекта в батче, т.е. тензор размера `batch_size x seqlen x dim`\n",
    "2. Получаем из исходных векторов векторы `query, key, value` с помощью линейного слоя. $W_q X, W_k X, W_v X$.\n",
    "    * **Важно:** не нужно делать три отдельных линейных слоя. Сделайте один линейный слой в три раза шире, затем после его применения разделите результат на три части с помощью метода `.chunk`. $W_{qkv} X$.\n",
    "3. Полученные query, key, value векторы делятся между \"головами\" аттеншна c помощью `.view`. Далее операции происходят для каждой головы отдельно.\n",
    "4. Нужно посчитать скалярные произведения всех запросов (queries) со всеми ключами (keys): $QV^T$.\n",
    "5. Заменить значения для паддинг токенов на очень маленькие (большие отрицательные), чтобы они не влияли на софтмакс:         \n",
    "    `attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000`\n",
    "\n",
    "6. Применить Dropout аттеншн скоров, который  выкидывает из аттеншна токены целиком.\n",
    "7. Поделить \"аттеншны скоры\" на корень из размерности векторов и взять софтмакс по ключам. Т.е. $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})$\n",
    "8. Посчитать контекстные векторы запросов $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})V$.\n",
    "9. Сконкатенировать контекстные векторы всех голов и применить линейный слой той же размерности и dropout.\n",
    "10. Сложить со входом **MultiHeadSelfAttention** слоя, применить layernorm: $\\text{layernorm}(x + \\text{dropout}(f(x)))$.\n",
    "\n",
    "**Про аттеншн маску:**\n",
    "* В полном виде аттеншн маска имеет размерность `batch_size x seqlen x seqlen`\n",
    "* У нас же если токен не паддинг, то его видят остальные токены, поэтому по сути вся информация содержится в матрице размера `batch_size x seqlen` с предикатом является ли токен паддингом\n",
    "* Эту матрицу размера `batch_size x seqlen` можно привести к виду `batch_size x seqlen x seqlen` операцией вида `attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]`\n",
    "\n",
    "**Вопросы:**\n",
    "1. Зачем нужно делить на корень из $d$ результаты скалярных произведений?\n",
    "2. Почему одно большое умножение на матрицу лучше, чем три маленьких?\n",
    "3. Что будет, если мы не будем заменять значения аттеншн скоров паддинг токенов на большие отрицательные значения?\n",
    "4. Какая вычислительная сложность (количество умножений) у операции **MultiheadSelfAttention**?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Деление на корень из $d$ (размерности векторов) используется для масштабирования скалярных произведений в аттеншне, что позволяет предотвратить слишком большие значения аттеншн-скоров, а это может привести к проблемам с градиентами. То есть обучение становится более стабильным.\n",
    "2. Одно большое умножение на матрицу вместо трех маленьких связано с оптимизацией и эффективностью. Так, мы можем выполнить все три проекции (query, key, value) одновременно, тем самым уменьшив количество операций.\n",
    "3. Если не заменить значения аттеншн-скоров для паддинг токенов на большие отрицательные значения, модель может учиться присваивать высокие веса этим токенам => пвнимание будет уделяться паддинг токенам, модель неправильно выучится.\n",
    "4. Вычислительная сложность операции self-attention в одной голове составляет $O(\\text{seqlen}^2 \\cdot d)$. Соответственно, для $h$ голов будет $O(h \\cdot \\text{seqlen}^2 \\cdot d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_attention_heads,\n",
    "            attention_probs_dropout_prob=0.,\n",
    "            dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        hidden_size: размерность эмбеддингов\n",
    "        num_attention_heads: количество голов аттеншна. Обычно выбирается как hidden_size / num_attention_heads = 64,\n",
    "            т.е. размерность векторов у одной головы 64\n",
    "        attention_probs_dropout_prob: вероятность дропаута для аттеншн скоров\n",
    "        dropout_prob: вероятность дропаута в конце слоя (перед суммой со входами)\n",
    "        eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.linear_qkv = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.linear_out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout_att = nn.Dropout(attention_probs_dropout_prob)\n",
    "        self.dropout_out = nn.Dropout(dropout_prob)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "\n",
    "    @property\n",
    "    def size_per_head(self):\n",
    "        \"\"\"\n",
    "            returns: размерность векторов для одной головы\n",
    "        \"\"\"\n",
    "        return self.head_size\n",
    "    \n",
    "    def forward(self, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "        embeddings: входные эмбеддинги\n",
    "        attention_mask: тензор из 0, 1 размерности batch_size x seqlen x seqlen\n",
    "            \n",
    "        returns: контекстные векторы\n",
    "        \"\"\"\n",
    "        B, L, D = embeddings.size()\n",
    "        \n",
    "        qkv = self.linear_qkv(embeddings)  # B x L x (D * 3)\n",
    "        qkv = qkv.view(B, L, 3, self.num_attention_heads, self.head_size)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4).contiguous()  # 3 x B x H x L x (D // H)\n",
    "        \n",
    "        query, key, value = qkv.chunk(3, dim=0)  # each B x H x L x (D // H)\n",
    "        \n",
    "        query = query.view(-1, L, self.head_size)  # (B * H) x L x (D // H)\n",
    "        key = key.view(-1, L, self.head_size)  # (B * H) x L x (D // H)\n",
    "        value = value.view(-1, L, self.head_size)  # (B * H) x L x (D // H)\n",
    "        \n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))  # (B * H) x L x L\n",
    "        \n",
    "        attention_mask = attention_mask.repeat(4, 1, 1)\n",
    "        attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000\n",
    "        \n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)  # (B * H) x L x L\n",
    "        attention_probs = self.dropout_att(attention_probs)  # (B * H) x L x L\n",
    "        \n",
    "        context = torch.matmul(attention_probs, value)  # (B * H) x L x (D // H)\n",
    "        context = context.view(-1, self.num_attention_heads, L, self.head_size)  # B x H x L x (D // H)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()  # B x L x H x (D // H)\n",
    "        context = context.view(B, L, -1)  # B x L x D\n",
    "        \n",
    "        out = self.linear_out(context)  # B x L x D\n",
    "        out = self.dropout_out(out)  # B x L x D\n",
    "        \n",
    "        output = embeddings + out\n",
    "        output = self.layernorm(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters: 263680.\n"
     ]
    }
   ],
   "source": [
    "tests.test_attention(MultiHeadSelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать **полносвязный слой** гораздо проще - $\\text{layernorm}(\\text{dropout}(W_2 f(W_1 x + b_1) + b_2) + x)$:\n",
    "1. Линейный слой, расширяющий входные векторы до *intermediate_size*, который традиционно равен 4 * hidden_size, т.е. происходит расширение в четыре раза\n",
    "2. Функция активации (больше вы их нигде в модели не увидите)\n",
    "3. Линейный слой, сужающий векторы обратно до *hidden_size*\n",
    "4. Dropout, сложение со входом полносвязного слоя, layernorm\n",
    "\n",
    "**Вопросы:**\n",
    "1. Что дает \"расширение\" первым линейным слоем? Нельзя ли делать линейный слой поменьше?\n",
    "2. Какая вычислительная сложность (количество умножений) у операции?\n",
    "3. Используются ли где-то еще в трансформере функции активации (если не считать softmax функцией активации)?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Расширение первым линейным слоем увеличивает сложность модели => модель имеет больше возможности уловить сложные зависимости между признаками входного вектора. Расширение позволяет работать с более высокоуровневыми признаками.\n",
    "2. Пусть размерность входного вектора равна hidden_size, а размерность выходного вектора равна intermediate_size (4 * hidden_size) => количество умножений будет равно hidden_size * intermediate_size = 4 * hidden_size * hidden_size.\n",
    "3. В трансформере функция активации применяется только в полносвязных слоях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            dropout_prob=0., \n",
    "            act_func='relu', \n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            intermediate_size: размерность промежуточно слоя. Обычно 4 * hidden_size\n",
    "            dropout_prob: вероятность дропаута перед суммой со входными представлениями\n",
    "            act_func: функция активации. Должны быть доступны gelu, relu\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Линейные слои\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        \n",
    "        # Dropout и Layernorm\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "        \n",
    "        # Функция активации\n",
    "        if act_func == 'relu':\n",
    "            self.act_func = F.relu\n",
    "        elif act_func == 'gelu':\n",
    "            self.act_func = F.gelu\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: входные эмбеддинги размерности batch_size x seqlen x hidden_size\n",
    "            \n",
    "            returns: преобразованные эмбеддинги той же размерности\n",
    "        \"\"\"\n",
    "        intermediate_output = self.act_func(self.linear1(embeddings))\n",
    "        intermediate_output = self.dropout(intermediate_output)\n",
    "        layer_output = self.linear2(intermediate_output)\n",
    "        \n",
    "        # Применяем dropout, добавляем к входу и применяем layernorm\n",
    "        output = self.layernorm(embeddings + self.dropout(layer_output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 526080.\n"
     ]
    }
   ],
   "source": [
    "tests.test_feedforward(FeedForward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **MultiHeadSelfAttention** и **Feedforward** в один блок энкодера. Они применяются последовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._multihead_attention = MultiHeadSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            dropout_prob=dropout_prob,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        self._feedforward = FeedForward(\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=intermediate_size,\n",
    "            act_func=act_func,\n",
    "            eps=eps,\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self._multihead_attention(x, attention_mask)\n",
    "        x = self._feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 789760.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_layer(BertLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объедините **BertEmbeddings** и произвольное заданное число **BertLayer** слоёв в один слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            hidden_size,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            input_dropout_prob=0.,\n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BertEmbeddings\n",
    "        self._embeddings = BertEmbeddings(\n",
    "            vocab_size=vocab_size, \n",
    "            hidden_size=hidden_size, \n",
    "            max_seqlen=max_seqlen,\n",
    "            dropout_prob=dropout_prob,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        # BertLayers\n",
    "        self._layers = nn.ModuleList([\n",
    "            BertLayer(\n",
    "                hidden_size=hidden_size,\n",
    "                intermediate_size=intermediate_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                dropout_prob=dropout_prob,\n",
    "                attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                act_func=act_func,\n",
    "                eps=eps\n",
    "            ) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "    \n",
    "    def get_token_embeddings(self):\n",
    "        return self._embeddings.get_token_embeddings()\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_mask(attention_mask):\n",
    "        return attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, token_type_ids=None):\n",
    "        # Embedding layer\n",
    "        embedding_output = self._embeddings(x, token_type_ids)\n",
    "        \n",
    "        # Expand attention mask\n",
    "        attention_mask = self.expand_mask(attention_mask) if attention_mask is not None else None\n",
    "        \n",
    "        # BertLayers\n",
    "        for layer in self._layers:\n",
    "            embedding_output = layer(embedding_output, attention_mask)\n",
    "        \n",
    "        return embedding_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 10905600.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предобучения (и для целевых задач) необходимо задать \"головы\" модели:\n",
    "\n",
    "Голова для **MLM** задачи выглядит как $W_2 \\text{layernorm} (f(W_1 x + b_1)) + b_2$:\n",
    "1. Линейный слой $d \\times d$\n",
    "2. Функция активации\n",
    "3. LayerNorm\n",
    "4. Линейный слой $d \\times |V|$, где $|V|$ --- размер словаря. **Важно:** в качестве матрицы, на которую происходит умножение при аффинном преобразовании, берется матрица эмбеддингов токенов.\n",
    "5. Функционал ошибки тоже будем считать сразу в голове, для него используется **nn.CrossEntropyLoss**: \n",
    "    * `self._criterion(preds.view(-1, self._vocab_size), labels.view(-1))`\n",
    "\n",
    "Чтобы использовать матрицу входных эмбеддингов вместо последнего линейного слоя в голове, можно использовать присваивание вида`self._decoder.weight = input_embeddings.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlmHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            hidden_act, \n",
    "            eps=1e-3, \n",
    "            ignore_index=-100, \n",
    "            input_embeddings=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            vocab_size: размер словаря\n",
    "            hidden_act: функция активации\n",
    "            eps: eps для layernorm\n",
    "            ignore_index: индекс таргета, который необходимо игнорировать при подсчете лосса\n",
    "            input_embeddings: слой с эмбеддингами токенов, для использования матрицы эмбеддингов вместо линейного слоя\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Линейные слои\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Функция активации\n",
    "        if hidden_act == 'relu':\n",
    "            self.hidden_act = F.relu\n",
    "        elif hidden_act == 'gelu':\n",
    "            self.hidden_act = F.gelu\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "        \n",
    "        # CrossEntropyLoss\n",
    "        self._criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        \n",
    "        # Установка матрицы эмбеддингов\n",
    "        if input_embeddings is not None:\n",
    "            self.linear2.weight = input_embeddings.weight\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги токенов\n",
    "            labels: истинные метки, т.е. изначальные индексы токенов\n",
    "            \n",
    "            returns: посчитанный лосс\n",
    "        \"\"\"\n",
    "        # Применяем линейные слои и функцию активации\n",
    "        mlm_output = self.hidden_act(self.linear1(hidden_states))\n",
    "        mlm_output = self.layernorm(mlm_output)\n",
    "        mlm_output = self.linear2(mlm_output)\n",
    "        \n",
    "        # Рассчитываем лосс\n",
    "        mlm_loss = self._criterion(mlm_output.view(-1, mlm_output.size(-1)), labels.view(-1))\n",
    "        \n",
    "        return mlm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7776304.\n"
     ]
    }
   ],
   "source": [
    "tests.test_mlm_head(MlmHead, BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Голова для **SOP**-задачи выглядит аналогично и в оригинальной статье называется \"pooler-слоем\":\n",
    "1. Берем скрытое представление CLS токена\n",
    "2. Линейный слой $d \\times d$\n",
    "3. Функция активации, причем в качестве функции активации используется гиперболический тангенс **nn.Tanh**\n",
    "4. Dropout\n",
    "5. Линейный слой\n",
    "6. Функционал ошибки (бинарная кросс-энтропия с логитами, **nn.BCEWithLogitsLoss**)\n",
    "\n",
    "Эту голову (кроме последнего линейного слоя) мы будем использовать также и для целевой задачи (классификации чеков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    CLS_POSITION = 0\n",
    "    CRITERION = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes=1, hidden_dropout_prob=0.):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            hidden_dropout_prob: вероятность дропаута\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Линейные слои\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        # Дропаут\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        \n",
    "        # Функция активации\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, permuted=None):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги\n",
    "            permuted: таргеты (были ли свапы сегментов). Если их нет, то необходимо выдать предсказания\n",
    "        \"\"\"\n",
    "\n",
    "        # Извлекаем CLS токен\n",
    "        cls_token = hidden_states[:, self.CLS_POSITION, :]\n",
    "        \n",
    "        # Применяем линейные слои, дропаут и функцию активации\n",
    "        cls_output = self.linear1(cls_token)\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        cls_output = self.activation(cls_output)\n",
    "        cls_output = self.linear2(cls_output)\n",
    "        \n",
    "        # Если permuted не задано, возвращаем предсказания\n",
    "        if permuted is None:\n",
    "            return cls_output\n",
    "        \n",
    "        # Иначе, считаем лосс для SOP задачи\n",
    "        sop_loss = self.CRITERION(cls_output.view(-1), permuted.float())\n",
    "        \n",
    "        return sop_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 66049.\n"
     ]
    }
   ],
   "source": [
    "tests.test_classifier_head(ClassifierHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **Bert**, **MlmHead** и **ClassifierHead** в единую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3, \n",
    "            ignore_index=-100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._mlm_head = MlmHead(\n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            act_func, \n",
    "            eps, \n",
    "            ignore_index, \n",
    "            input_embeddings=self._backbone.get_token_embeddings()\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(\n",
    "            hidden_size, \n",
    "            hidden_dropout_prob=hidden_dropout_prob, \n",
    "            num_classes=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask, labels, permuted, token_type_ids=None):\n",
    "        hidden_states = self._backbone(x, attention_mask, token_type_ids)\n",
    "        mlm_loss = self._mlm_head(hidden_states, labels)\n",
    "        sop_loss = self._classifier_head(hidden_states, permuted)\n",
    "        # в оригинальном BERT лоссы MLP и NSP используются с равными весами\n",
    "        return 0.5 * mlm_loss + 0.5 * sop_loss, {'MLM': mlm_loss, 'SOP': sop_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения гиперпараметров:\n",
    "* для успешного выполнения задания достаточно архитектуры bert-mini: `hidden_size=256`, `num_hidden_layers=4`, в качестве функции активации можно использовать `gelu`\n",
    "* стандартные практики: `intermediate_size = 4 * hidden_size`, `num_attention_heads = hidden_size // 64`\n",
    "* в оригинальной статье везде dropout равен 0.1, но для bert-mini модели можно попробовать значения поменьше. Вопрос - почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(\n",
    "    hidden_size = 256, \n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    max_seqlen = ds._maxlen,\n",
    "    num_hidden_layers = 4,\n",
    "    intermediate_size = 4*256,\n",
    "    num_attention_heads = 256//64,\n",
    "    act_func='relu',\n",
    "    input_dropout_prob=0.,\n",
    "    hidden_dropout_prob=0., \n",
    "    attention_probs_dropout_prob=0.,\n",
    "    eps=1e-3, \n",
    "    ignore_index=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Какая часть модели содержит наибольшее количество параметров? Эмбеддинги, аттеншн, полносвязные слои, голова?\n",
    "2. Зачем объединять параметры в голове и параметры матрицы эмбеддингов?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Оптимизация (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации будем использовать **AdamW**, отличия которого от ванильного **Adam** можно почитать, например, [вот здесь](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n",
    "\n",
    "Параметры модели, передаваемые в оптимизатор, следует поделить на две группы с помощью `model.named_parameters()`:\n",
    "1. Все `bias` и `layernorm` слои, присутствующие в модели (их можно выцепить по названию). Для них $l_2$ регуляризацию стоит выключить, т.е. поставить `weight_decay=0`\n",
    "2. Оставшиеся слои, для которых регуляризация не нужна.\n",
    "\n",
    "\n",
    "**Вопрос:** почему $l_2$ регуляризацию не используют для bias'ов? Для layernorm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "        model: инициализированная модель\n",
    "        weight_decay: коэффициент l2 регуляризации\n",
    "        \n",
    "        returns: оптимизатор\n",
    "    \"\"\"\n",
    "    decayed_parameters, not_decayed_parameters = [], []\n",
    "    \n",
    "    # Разделение параметров на две группы\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' in name or 'layernorm' in name:\n",
    "            decayed_parameters.append(param)\n",
    "        else:\n",
    "            not_decayed_parameters.append(param)\n",
    "            \n",
    "    grouped_parameters = [\n",
    "        {'params': decayed_parameters, 'weight_decay': weight_decay},\n",
    "        {'params': not_decayed_parameters, 'weight_decay': 0.}\n",
    "    ]\n",
    "\n",
    "    return torch.optim.AdamW(grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimizer(get_optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит типичное расписание lr для трансформеров:\n",
    "\n",
    "<img src=\"images/lr.png\" width=300 height=300 />\n",
    "\n",
    "Почему мы сразу не стартуем с большого значения lr? Для больших архитектур трансформера модель разойдется, произойдет взрыв градиентов. Постепенно же увеличить lr до большого значения — можно. Процедуру линейного увеличения lr до какого-то пикового значения называют `linear warmup`.\n",
    "\n",
    "Реализуйте такое \"треугольное\" расписание для learning rate в предложенном шаблоне.\n",
    "\n",
    "**Вопрос:** а зачем нужно убывание learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            optimizer, \n",
    "            init_lr, \n",
    "            peak_lr, \n",
    "            final_lr, \n",
    "            num_warmup_steps, \n",
    "            num_training_steps\n",
    "    ):\n",
    "        \"\"\"\n",
    "            optimizer: оптимизатор\n",
    "            init_lr: начальное значение learning rate\n",
    "            peak_lr: пиковое значение learning rate\n",
    "            final_lr: финальное значение lr\n",
    "            num_warmup_steps: количество шагов разогрева (сколько шагов идем от начального до пикового значения)\n",
    "            num_training_steps: количество шагов обучения (количество батчей x количество эпох)\n",
    "            \n",
    "        \"\"\"\n",
    "        self._optimizer = optimizer\n",
    "        self._step = 0\n",
    "        \n",
    "        # Инициализация параметров\n",
    "        self._init_lr = init_lr\n",
    "        self._peak_lr = peak_lr\n",
    "        self._final_lr = final_lr\n",
    "        self._num_warmup_steps = num_warmup_steps\n",
    "        self._num_training_steps = num_training_steps\n",
    "        self._lr = self._init_lr\n",
    "        \n",
    "        # Шаг изменения learning rate\n",
    "        self._warmup_schedule = [(self._peak_lr - self._init_lr) / self._num_warmup_steps, \n",
    "                                 (self._final_lr - self._peak_lr) / (self._num_training_steps - self._num_warmup_steps)]\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "            Меняет learning rate для оптимизатора\n",
    "            \n",
    "            Поменять learning rate для группы параметров в оптимизаторе можно присваиванием вида param_group['lr'] = lr\n",
    "        \"\"\"\n",
    "        # Обновление learning rate\n",
    "        if self._step < self._num_warmup_steps:\n",
    "            self._lr += self._warmup_schedule[0]\n",
    "        elif self._step < self._num_training_steps:\n",
    "            self._lr += self._warmup_schedule[1]\n",
    "        \n",
    "        # Применение нового learning rate к оптимизатору\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = self._lr\n",
    "            \n",
    "        self._step += 1\n",
    "        \n",
    "    def get_last_lr(self):\n",
    "        \"\"\"\n",
    "            returns: текущий learning rate оптимизатора. Нужно для логгирования\n",
    "        \"\"\"\n",
    "        return [param_group['lr'] for param_group in self._optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_scheduler(Scheduler, get_optimizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(\n",
    "    optimizer = optimizer,\n",
    "    init_lr = 3e-4,\n",
    "    peak_lr = 8e-4,\n",
    "    final_lr = 0,\n",
    "    num_warmup_steps = 250, \n",
    "    num_training_steps = 6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От запуска обучения нас отделяет только создание `Trainer`. От объектов класса `Trainer` требуется, чтобы:\n",
    "* логгировался лосс на каждом батче (`torch.utils.tensorboard.SummaryWriter`, `writer.add_scalar`)\n",
    "* клипались и логгировались нормы градиентов при каждом шаге спуска (`orch.nn.utils.clip_grad_norm_` возвращает нормы градиентов)\n",
    "* логгировались значения learning rate\n",
    "* была поддержана аккумуляция градиентов, нужная для эмуляции больших батчей\n",
    "\n",
    "При предобучении не нужно использовать какую-либо форму валидации, достаточно смотреть на батч лосс.\n",
    "\n",
    "Предлагается также для ускорения обучения использовать mixed precision из библиотеки `apex`:\n",
    "* перед обучением необходимо вызвать строчку вида `model, optimizer = amp.initialize(model, optimizer, opt_level='O1')`\n",
    "* при обучении `.backward()` надо делать в контекстном менеджере:     \n",
    "   `with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()`\n",
    "        \n",
    "Что такое аккумуляция градиентов:\n",
    "* При использовании Adam в видеопамяти необходимо хранить градиенты и квадраты частных производных\n",
    "* При подсчете градиента по очередному батчу необязательно сразу делать шаг спуска, можно запомнить градиент, а затем посчитать градиент по другому батчу c теми же параметрами модели\n",
    "* Теперь эти два градиента можно сложить и получить градиент, который был посчитан как будто по одному большому батчу (составленному из этих двух). Сэмулировали большой батч. В данном случае количество шагов аккумуляции равно двум.\n",
    "* В данном случае количество шагов аккумуляции равно двум.\n",
    "\n",
    "Зачем нужны большие батчи? Обучение быстрее, оценки градиента точнее, позволяет увеличивать learning rate. Например, при предобучении авторы RoBERTA значительно увеличили размер батча по сравнению с ванильным BERT и получили прирост к качеству решения целевых задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/apex/__init__.py:13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterfaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (IAuthenticationPolicy,\n\u001b[1;32m     10\u001b[0m                                 IAuthorizationPolicy,\n\u001b[1;32m     11\u001b[0m                                 ISessionFactory)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecurity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NO_PERMISSION_REQUIRED\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnencryptedCookieSessionFactoryConfig\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asbool\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ApexAuthSecret,\n\u001b[1;32m     17\u001b[0m                              ApexSessionSecret)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            num_accum_steps=1,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._device = device\n",
    "        self._num_accum_steps = num_accum_steps\n",
    "        self._logdir = logdir\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._n_batch = 0\n",
    "        self._n_epoch = 0\n",
    "        \n",
    "        # Логгирование с использованием TensorBoard\n",
    "        if logdir is not None:\n",
    "            if os.path.exists(logdir):\n",
    "                shutil.rmtree(logdir)\n",
    "            os.makedirs(logdir)\n",
    "            self._writer = SummaryWriter(logdir)\n",
    "        else:\n",
    "            self._writer = None\n",
    "\n",
    "    def train(self, dataloader, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            self._train_step(dataloader)\n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        self._model.train()\n",
    "        \n",
    "        # Активация mixed precision\n",
    "        self._model, self._optimizer = amp.initialize(self._model, self._optimizer, opt_level='O1')\n",
    "\n",
    "        # Старт нового батча\n",
    "        for batch in dataloader:\n",
    "            # Перенос данных на устройство\n",
    "            inputs = {key: value.to(self._device) for key, value in batch.items() if key != 'permuted'}\n",
    "            labels = batch['permuted'].to(self._device)\n",
    "            \n",
    "            # Подсчет лосса\n",
    "            loss = self._model(inputs, labels)\n",
    "            loss = loss / self._num_accum_steps  # усреднение лосса для аккумуляции\n",
    "            \n",
    "            # Применение маски паддинга\n",
    "            attention_mask = (inputs['input_ids'] != self._pad_token_id).float()\n",
    "\n",
    "            # Mixed precision: подсчет градиентов\n",
    "            with amp.scale_loss(loss, self._optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            \n",
    "            # Логгирование норм градиентов\n",
    "            if self._n_batch % self._num_accum_steps == 0:\n",
    "                if self._max_grad_norm is not None:\n",
    "                    # Клиппинг градиентов\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(self._optimizer), self._max_grad_norm)\n",
    "                \n",
    "                # Логгирование норм градиентов\n",
    "                if self._writer is not None:\n",
    "                    grad_norms = defaultdict(list)\n",
    "                    for name, param in self._model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            grad_norm = param.grad.norm().item()\n",
    "                            grad_norms[name].append(grad_norm)\n",
    "                    \n",
    "                    for name, norms in grad_norms.items():\n",
    "                        self._writer.add_scalar(f'grad_norm/{name}', sum(norms) / len(norms), self._n_batch)\n",
    "\n",
    "            # Оптимизация каждые num_accum_steps батчей\n",
    "            if self._n_batch % self._num_accum_steps == self._num_accum_steps - 1:\n",
    "                # Шаг оптимизатора\n",
    "                self._optimizer.step()\n",
    "                self._optimizer.zero_grad()\n",
    "                \n",
    "                # Шаг learning rate scheduler\n",
    "                self._scheduler.step()\n",
    "\n",
    "            # Логгирование лосса\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('loss', loss.item(), self._n_batch)\n",
    "\n",
    "            self._n_batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите и сохраните предобученную модель с помощью `torch.save`. \n",
    "\n",
    "**Важно:** тензорборд логи успешного обучения необходимо сложить в архив и приложить вместе с решенным заданием.\n",
    "\n",
    "Про гиперпараметры:\n",
    "* `weight_decay` - $0.1, 0.01, 0.001$ и т.д.\n",
    "* расписание lr - bert-mini не очень чувствителен к линейному вормапу, поэтому существенное влияние оказывают только пиковое и финальное значение lr. Пиковое значение стоит поискать где-то в масштабе 1e-3 - 1e-4, финальный lr можно сделать очень маленьким.\n",
    "* конкретное значение для клиппинга нормы особо ни на что не влияет, как правило (и в оригинальной статье тоже) его всегда ставят единицой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(dl, n_epochs=...)\n",
    "    \n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    'pretrained_weights.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После предобучения вам придется перезапустить ноутбук и снова перепрогнать блоки, нужные для дообучения. Использование apex'а ломает обучение других моделей (которые не передавались в `amp.initialize`) в одном запуске. Если не перезапустить, скор получится гораздо хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Дообучение (5 баллов)\n",
    "\n",
    "Самая сложная часть уже позади, осталось чуть-чуть :)\n",
    "\n",
    "Так как для дообучения доступно гораздо меньше данных, оно занимает гораздо меньше времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data['split'] == 'train'].reset_index(drop=True).copy()\n",
    "val = data.loc[data['split'] == 'val'].reset_index(drop=True).copy()\n",
    "test = data.loc[data['split'] == 'test'].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет для дообучения выглядит стандартно: нужно токенизировать и запомнить тексты и соответствующие им метки, и затем в методе `__getitem__` их выдавать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            texts, \n",
    "            targets, \n",
    "            tokenizer,\n",
    "            maxlen, \n",
    "            presort=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "            texts: list of strings. Тексты чеков\n",
    "            targets: list of ints. Категории товаров\n",
    "            tokenizer: токенизатор\n",
    "            maxlen: максимальная длина текста\n",
    "            presort: отсортировать тексты по длине\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: input_ids - индексы токенов токенизированного текста, target - категория\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "        return input_ids, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасеты для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FinetuneDataset(\n",
    "    train['text'].values, \n",
    "    train['label'].values, \n",
    "    maxlen=..., \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_ds = FinetuneDataset(\n",
    "    val['text'].values, \n",
    "    val['label'].values, \n",
    "    maxlen=..., \n",
    "    tokenizer=tokenizer, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллатор для дообучения делает только паддинг и конвертацию таргетов в тензоры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте даталоадеры для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = ...\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    ),\n",
    "    'eval': DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели теперь отсутствует MLM голова, а вместо SOP задачи голова классификации решает задачу определения категорий товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFinetuneModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            num_classes,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(hidden_size, num_classes, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        hidden_states = self._backbone(x, attention_mask)\n",
    "        return self._classifier_head(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте ту же архитектуру, которую вы выбрали при предобучении. Количество классов - 96:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertFinetuneModel(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузить предобученные веса можно с помощью следующей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(self, path):\n",
    "    found = []\n",
    "    with open(path, 'rb') as f:\n",
    "        weights = torch.load(f)\n",
    "    for name, param in weights.items():\n",
    "        if name in self.state_dict():\n",
    "            if param.shape == self.state_dict()[name].shape:\n",
    "                self.state_dict()[name].copy_(param)\n",
    "                found.append(name)\n",
    "\n",
    "    return found\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте оптимизатор и расписание лр. Про гиперпараметры:\n",
    "* при дообучении используют маленький batch_size $\\in \\{32, 64\\}$\n",
    "* маленький learning rate:  $\\{1e-5, 2e-5, 4e-5\\}$ для больших моделей, для моделей вида bert-mini можно использовать и побольше: $\\{1e-4, 2e-4, 4e-4\\}$ \n",
    "* финальное значение все также маленькое\n",
    "* вормап можно делать где-то 0.06 от всех шагов обучения\n",
    "* количество эпох для дообучения - больше шести здесь не нужно\n",
    "* weight decay здесь потенциально можно использовать побольше, чем при предобучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=...)\n",
    "scheduler = Scheduler(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось создать пайплайн обучения:\n",
    "* apex использовать не нужно, дообучение быстрое и не требует больших батчей\n",
    "* аккумуляция градиентов не нужна т.к. батчи  маленькие\n",
    "* лосс теперь считается вне модели, в Trainer нужно использовать torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneTrainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "    def train(self, dataloaders, n_epochs, scorer=None):\n",
    "        \"\"\"\n",
    "            dataloaders: dict of dataloaders, keys 'train', 'eval' should be present.\n",
    "            n_epochs: int. Num epochs to train for.\n",
    "            scorer: takes trainer, outputs metric name and value as a tuple.\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            val_loss = self._eval_step(dataloaders['eval'])\n",
    "\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval', val_loss, global_step=self._n_epoch)\n",
    "                \n",
    "                if scorer is not None:\n",
    "                    name, value = scorer(self)\n",
    "                    self._writer.add_scalar(name, value, global_step=self._n_epoch)\n",
    "                    \n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: training dataloader.\n",
    "            \n",
    "            returns: train_loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "\n",
    "\n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: evaluation dataloader.\n",
    "            \n",
    "            returns: eval loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: inference dataloader. Should not have targets.\n",
    "            \n",
    "            returns: np.array c предсказанными категориями\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для мониторинга целевой метрики используйте предоставленный scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, maxlen, tokenizer):\n",
    "        \"\"\"\n",
    "            texts: list of str. Сырые тексты чеков\n",
    "            maxlen: максимальная длина текста\n",
    "            tokenizer: токенизатор\n",
    "        \"\"\"\n",
    "        self._texts = [tokenizer(text) if tokenizer is not None else text for text in texts]\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: тензор из индексов токенов токенизированного текста\n",
    "        \"\"\"\n",
    "        text = self._texts[idx]\n",
    "        if self._maxlen is not None:\n",
    "            text = text[:self._maxlen]\n",
    "        return torch.tensor(text, dtype=torch.long)\n",
    "    \n",
    "def make_scorer(texts, targets, tokenizer, maxlen):\n",
    "    inference_ds = InferenceDataset(texts, maxlen=maxlen, tokenizer=tokenizer)\n",
    "    inference_dl = DataLoader(inference_ds, batch_size=32, shuffle=False, collate_fn=inference_collate_fn)\n",
    "    def get_score(trainer):\n",
    "        preds = trainer.predict(inference_dl)\n",
    "        return 'f1', f1_score(targets, preds, average='weighted')\n",
    "    return get_score\n",
    "\n",
    "\n",
    "inference_collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "val_scorer = make_scorer(val['text'].values, val['label'].values, tokenizer, maxlen=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор на валидационной выборке до обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scorer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(dataloaders, n_epochs=..., scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полного балла за задание является получение на тесте значения метрики $\\geqslant 0.7$. Скор на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=...)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забудьте также приложить вместе со сделанным заданием тензорборд дообучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Попробуйте также обучить модель без предобученных весов (просто закомментировав загрузку весов). Насколько сильно просело качество?\n",
    "2. Влияет ли длительность предобучения (количество эпох) как-то существенно на дообучение, или достаточно одной эпохи?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть. Большие модели (максимум 3 балла)\n",
    "\n",
    "Предлагается обучить модель побольше:\n",
    "* `hidden_size` $\\in \\{512, 768, 1024\\}$\n",
    "* `num_hidden_layers` $\\in \\{8, 12, 24\\}$\n",
    "\n",
    "Например, BERT-base архитектура выглядит как `hidden_size=768, num_hidden_layers=12`.\n",
    "\n",
    "Для большой модели придется также использовать другие гиперпараметры - нужен learning rate поменьше, weight decay побольше, дропаут больше. Возможно потребуется больше эпох предобучения.\n",
    "\n",
    "За выполнение этой части можно получить **до пяти бонусных баллов**, бонус зависит от полученных на тесте значений метрики (должно быть видно существенное улучшение)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
